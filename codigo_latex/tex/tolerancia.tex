\section{Tolerancia a Fallas}

\subsection{Introducción}

En esta sección se detallan los mecanismos implementados en el Sistema Distribuido para tolerar distintos escenarios de falla.  
El objetivo principal de estas implementaciones es garantizar la \textbf{consistencia} y la \textbf{robustez} del sistema, priorizando estos atributos por sobre la eficiencia en términos de rendimiento.

En cuanto a la arquitectura general, no se presentan modificaciones en la topología previamente documentada.  
El único agregado corresponde a la incorporación de un conjunto de nodos denominados \textit{Health Checkers}, encargados de consultar de forma recurrente el estado de cada nodo del sistema, con el fin de validar su correcto funcionamiento o, en caso de detectar fallas, iniciar procesos de recuperación para restablecer las instancias caídas.

\subsection{Health Checkers}

Los nodos \textit{Health Checkers} se encargan de monitorear continuamente que todos los nodos encargados del procesamiento para la generación de resultados se encuentren activos.  
Este mecanismo está implementado mediante múltiples instancias dispuestas en una topología en anillo, con el fin de garantizar la tolerancia a fallas y la alta disponibilidad del propio sistema de monitoreo.

Uno de los nodos cumple el rol de \textbf{Líder}, siendo el responsable de coordinar las validaciones periódicas.  
En caso de que el líder falle, se ejecuta automáticamente un algoritmo de elección que designa un nuevo líder, el cual retoma las tareas del anterior.

El proceso de verificación consiste en el envío periódico de mensajes de tipo \textit{Ping} a cada nodo del sistema.  
Si el nodo responde con un \textit{ACK}, se considera activo; En caso contrario, el líder inicia el proceso de recuperación de esa instancia.

El servidor central también participa de este esquema de \textit{heartbeat}.
Por cada cliente que levanta, registra el identificador de sesión en un archivo persistente.  
Si el servidor se cae y luego se reinicia, utiliza esa información para notificar a todos los controladores que deben limpiar el estado asociado a las sesiones previamente activas, evitando que queden residuos de consultas incompletas.  
De forma análoga, cuando el servidor detecta que un cliente finaliza o se desconecta definitivamente, envía una orden de limpieza sólo para esa sesión, de modo que se liberen los recursos correspondientes en todos los nodos sin afectar las consultas del resto de los clientes.

\subsubsection{Topología del Sistema de Health Checkers}

A continuación se dejan unos diagramas que representan:

\begin{enumerate}
    \item La topología de anillo de los Health Checkers.
    \item La interacción entre el líder y los otros nodos Health Checkers.
    \item La interacción entre el líder y los nodos del Sistema Distribuido.
\end{enumerate}

\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.16, keepaspectratio]{ring.png}
  \caption{Topología de anillo de los Health Checkers}
  \label{fig:healthcheckers}
\end{figure}

\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.16, keepaspectratio]{health1.png}
  \caption{Pings y ACKs que manda el Líder a los otros Health Checkers}
  \label{fig:healthcheckers}
\end{figure}

\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.16, keepaspectratio]{health2.png}
  \caption{Pings y ACKs que mandan los Health Checkers al Líder}
  \label{fig:healthcheckers}
\end{figure}

\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.4, keepaspectratio]{health3.png}
  \caption{Pings y ACKs que manda el Líder a los nodos del Sistema Distribuido}
  \label{fig:healthcheckers}
\end{figure}

\newpage

\subsection{Cambio de comportamiento en los nodos}

Se introdujeron modificaciones significativas en el funcionamiento de todos los nodos del sistema que consumen mensajes desde las colas de \textit{RabbitMQ}.  
El objetivo de estos cambios es fortalecer la robustez del sistema ante fallas parciales y garantizar la consistencia de los datos en todo momento.

Cada nodo, al recibir (``popear'') un mensaje desde la cola correspondiente, ejecuta su lógica de procesamiento, le envía el mensaje a la siguiente cola de el próximo nodo, escribe el mensaje o el estado actualizado en su volumen persistente, y finalmente envía el \textit{ACK} a \textit{RabbitMQ}.  
Este orden de operaciones —procesamiento, envío, persistencia y confirmación— es esencial para evitar duplicaciones o pérdidas de mensajes ante fallas inesperadas.  

Gracias a la semántica de confirmación de \textit{RabbitMQ}, si un mensaje es consumido pero no se envía el \textit{ACK}, dicho mensaje se reencola automáticamente al inicio de la cola.  
De esta forma, ante una caída, el nodo podrá retomar su ejecución desde el último estado persistido o reprocesar el mensaje que quedó pendiente, garantizando así la consistencia del flujo.

\textbf{Observación:}  
Todos los archivos generados se almacenan en volúmenes persistentes asociados a los contenedores de cada nodo.  
Esto permite que, incluso tras una caída o reinicio, el nodo conserve su información y pueda recuperar su estado previo sin pérdida de datos.

\subsubsection{Nodos Stateless}

Los nodos \textit{Stateless} procesan mensajes de manera independiente, sin mantener un estado acumulativo entre ellos.  
Su flujo de ejecución se compone de las siguientes etapas:

\begin{enumerate}
    \item Recepción del mensaje desde la cola de entrada.
    \item Procesamiento del mensaje (limpieza, filtrado u otra operación específica).
    \item Envío del resultado al siguiente nodo.
    \item Escritura del mensaje procesado en un archivo local del volumen persistente, para disponer de una copia de respaldo.
    \item Envío del \textit{ACK} a \textit{RabbitMQ}.
\end{enumerate}

La escritura en disco se realiza \textbf{únicamente al final del proceso}, luego de haber enviado el mensaje.  
Este orden es crítico, ya que evita que un mensaje pueda ser reenviado o computado más de una vez, garantizando la consistencia global del sistema.

\subsubsection{Nodos Stateful (Joins)}

Los nodos \textit{Joins} presentan un comportamiento intermedio entre los nodos con y sin estado.  
Durante su ejecución inicial, reciben información de configuración proveniente del servidor, la cual se almacena en su volumen local y se utiliza de manera incremental al procesar los siguientes mensajes.

Una vez recibida toda la información del servidor, el nodo pasa a comportarse de forma análoga a un nodo \textit{Stateless}:  
Procesa, envía, registra y confirma cada mensaje de manera independiente.

Para facilitar la recuperación ante fallas, una vez completada la fase de inicialización, el nodo \textit{Join} registra un indicador distintivo (Por ejemplo, una marca de finalización) en su archivo local.  
De esta forma, si el nodo se reinicia, puede detectar fácilmente si ya recibió todos los datos del servidor o si aún resta información por almacenar.

Como optimización adicional, los \textit{Joiners} persisten la información base separada por sesión.  
Cada consulta mantiene su propio archivo de estado, lo que permite recomponer de forma localizada la información de un cliente en caso de caída, y evita que el estado de una sesión afecte o contamine el de las demás.

\newpage

\subsubsection{Nodos Stateful con estado acumulativo (Sorts, Sums, Counts)}

\textbf{Funcionamiento inicial del nodo}

Los nodos con estado acumulativo —como \textit{Sorts}, \textit{Sums} y \textit{Counts}— mantienen información agregada entre mensajes.  
Su comportamiento fue rediseñado para maximizar la confiabilidad y optimizar la recuperación ante fallas.

Cada nodo sigue el siguiente flujo:

\begin{enumerate}
    \item Recibe un mensaje desde la cola correspondiente.
    \item Actualiza su estado interno con la información del mensaje (operación de cómputo o agregación).
    \item Escribe el mensaje procesado en su archivo local (uno por cada nodo antecesor).
    \item Envía el \textit{ACK} a \textit{RabbitMQ}.
    \item Continúa con el siguiente mensaje.
\end{enumerate}

Cada cierto número de mensajes $n$, configurable según la carga del nodo o la probabilidad de falla estimada, se realiza un \textbf{backup o checkpoint} del estado actual.  
Este se guarda en un archivo separado dentro del mismo volumen, y actúa como punto de restauración ante fallas.

\textbf{Envío del reporte generado hacia el próximo nodo}

Finalmente, una vez completado el procesamiento de todos los mensajes correspondientes a una consulta o cliente, el nodo realiza el cálculo del estado final acumulado antes de enviar cualquier resultado al siguiente nodo de la secuencia.  
Ese estado final se persiste en el archivo de \textbf{backup/checkpoint}, garantizando la existencia de una versión consistente desde la cual se puedan regenerar los mensajes de salida.

Posteriormente, se procede al envío del estado al nodo siguiente.  
Entre cada mensaje que se emite (Por ejemplo, cuando el resultado debe fragmentarse en varios mensajes), el nodo actualiza y persiste el estado intermedio asociado a esa salida.  
De esta forma, si se produce una caída durante el envío, al reiniciarse puede reconstruir exactamente los mismos mensajes a partir del estado almacenado, sin introducir variaciones en el contenido.

Una vez completado el envío de todos los fragmentos, el nodo registra en su volumen una marca o indicador que señala que el estado correspondiente a esa consulta o cliente ya fue transmitido correctamente.  
Esto permite que, en caso de una caída y posterior recuperación, el nodo identifique que el resultado ya fue enviado y evite duplicaciones o recomputaciones innecesarias.

En los casos en que el envío del estado final requiera dividir la información en múltiples mensajes, se aplica la misma lógica de procesamiento y persistencia que para el resto de los mensajes:  
cada fragmento enviado se genera de forma determinista a partir del estado persistido y se escribe previamente en un volumen dedicado al manejo de dichos estados parciales.  
Así, si un mensaje debe reenviarse por un fallo en la confirmación, el contenido es idéntico al original y puede ser detectado como duplicado por el nodo receptor usando el \texttt{msg\_uuid} o los registros locales.

\textbf{Consideraciones}

Para mantener el uso eficiente del almacenamiento, tras cada backup se eliminan o marcan los registros previos como \textit{COMMIT}, indicando que los mensajes anteriores ya fueron computados y persistidos en el estado actual.

Cada nodo mantiene archivos separados por cada uno de sus antecesores.  
Esto permite acceder de manera directa al último mensaje procesado por cada fuente, manteniendo la trazabilidad y la consistencia del flujo completo.  
Se asume además que todas las escrituras en disco son \textbf{atómicas}, garantizando que sólo existen dos posibles resultados: escritura completa o ausencia total de la misma, eliminando la posibilidad de estados intermedios inconsistentes.

\newpage

\subsubsection{Procesos e Hilos por nodo}

En cuanto a la arquitectura interna de cada nodo, se implementó un modelo basado en múltiples hilos y/o procesos de ejecución para optimizar el procesamiento y posibilitar la gestión de mensajes en paralelo.

A continuación se describen mediante a diagramas los distintos modelos implementados según el tipo de nodo:

\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.12, keepaspectratio]{controller_processes.png}
  \caption{Procesos en los Controladores}
  \label{fig:stateless_model}
\end{figure}

\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.22, keepaspectratio]{health_processes.png}
  \caption{Procesos e hilos en los Health Checkers}
  \label{fig:stateless_model}
\end{figure}

\FloatBarrier
\begin{figure}[H]
  \centering
  \rotatebox{270}{%
    \includegraphics[
      width=0.95\textheight,
      keepaspectratio
    ]{img/server_processes.png}%
  }
  \caption{Procesos en el Servidor Central}
  \label{fig:robustez}
\end{figure}

\newpage

\subsection{Cambios en el protocolo}

La nueva implementación introduce un encabezado (\textit{header}) ligero que se agrega sobre el paquete existente, siguiendo un enfoque análogo al apilado de cabeceras entre capas de enlace y red en redes tradicionales. Este encabezado incorpora dos campos adicionales:

\begin{itemize}
  \item \textbf{UUID del mensaje} (\texttt{msg\_uuid}): Generado por el servidor emisor inicial y \textit{consistente durante toda la vida del mensaje} a través de la tubería de procesamiento. En los nodos que generan reportes/outputs finales, se genera un \textit{nuevo} UUID para el artefacto resultante, el cual se usa a partir de ahí para su trazabilidad.
  \item \textbf{Identificador del nodo origen} (\texttt{src\_node\_id}): Indica el nodo que produjo el mensaje. Se utiliza en el receptor para dirigir la persistencia al volumen/archivo correspondiente a ese origen, preservando trazabilidad y consistencia.
\end{itemize}

\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.18, keepaspectratio]{mensajes.png}
  \caption{Estructura de los mensajes}
  \label{fig:estructura_mensajes}
\end{figure}

\paragraph{Balanceo determinista basado en hash}
Los nodos que antes distribuían carga con \textit{Round Robin} (contador) migran a \textbf{hash por \texttt{msg\_uuid}}. Este cambio evita desbalances e inconsistencias cuando un nodo cae y reinicia su contador:
\begin{itemize}
  \item Con \textit{Round Robin}, un reinicio cambia el punto del ciclo y el mismo mensaje podría enrutarse a un destino distinto.
  \item Con \textbf{hash(msg\_uuid)}, a igual UUID se obtiene el mismo destino (consistentemente), incluso si el nodo se reinicia. Esto preserva afinidad de mensaje y localización esperada del estado.
\end{itemize}

\paragraph{Compatibilidad y consistencia}
El encabezado es aditivo, no rompe el formato del payload previo. La escritura local sigue siendo atómica, y la lógica de ACK de \textit{RabbitMQ} permanece inalterada: el ACK sólo se emite cuando el procesamiento y la persistencia asociada se completan, manteniendo idempotencia extremo a extremo.

\newpage

\subsection{Escenarios de Falla}

A continuación se detallan los distintos escenarios de falla contemplados durante el diseño de los mecanismos de tolerancia y recuperación del sistema.  
En todos los casos, se garantiza tanto la continuidad operativa ante la caída de uno o más nodos, como la consistencia de los resultados y del flujo de mensajes.

\subsubsection{Falla en los Health Checkers}

Ante la caída de algún nodo del sistema de \textit{Health Checkers}, pueden presentarse dos situaciones:

\begin{itemize}
    \item \textbf{Falla del líder:}  
    Se ejecuta un proceso de elección para designar un nuevo líder, el cual se reconecta con los nodos activos del sistema y retoma las tareas pendientes.

    \item \textbf{Falla de un nodo no líder:}  
    La instancia se remueve temporalmente del anillo para mantener la coherencia de la topología.
\end{itemize}

En ambos casos, se intenta levantar nuevamente la instancia caída para restablecer el nivel de redundancia definido y mantener la alta disponibilidad del sistema.

\subsubsection{Falla en los Nodos Stateless y Nodos Join}

El comportamiento ante fallas es similar al de los nodos \textit{Join}:

\begin{itemize}
    \item \textbf{Falla antes del \textit{ACK}:}  
    \textit{RabbitMQ} reencola el mensaje, que será procesado nuevamente cuando el nodo se recupere.
    Como esta es la última operación antes de popear otro mensaje, no existe otro instante de falla a considerar.
\end{itemize}

\subsubsection{Falla en los Nodos Statefull}

Dependiendo del tipo de nodo, los mecanismos de recuperación difieren:

\paragraph{Nodos con estado acumulativo (Sorts, Sums, Counts):}
\begin{itemize}
    \item \textbf{Si el nodo falla antes de procesar el mensaje}:  
    El mensaje se reencola automáticamente por \textit{RabbitMQ}, y al reiniciarse, el nodo lo vuelve a procesar desde su archivo o desde el backup más reciente.

    \item \textbf{Si falla después de procesar el mensaje pero antes del \textit{ACK}}:  
    El mensaje se reencola, y al recuperarse, el nodo detecta en su registro que el mensaje ya fue computado, evitando duplicaciones.

    \item \textbf{Si falla durante la acumulación}:  
    Al reiniciarse, el nodo restaura su estado desde el último backup disponible y continúa procesando desde el siguiente mensaje.

    \item \textbf{Si falla en el proceso de envío del reporte hacia el próximo nodo}:  
    Al volver a despertar, se retoma desde el último mensaje enviado, para garantizar la consistencia del sistema, evitando mensajes duplicados.
    Para identificar los mensajes, el nodo que genera el reporte asigna los IDs correspondientes, haciendose responsable de su correctitud.
\end{itemize}

\newpage

\subsubsection{Fallas Desestimadas}

Durante el análisis se identificaron otros posibles tipos de falla:

\begin{itemize}
    \item Falla en el \textbf{Servidor Central}.
    \item Falla en los \textbf{Clientes}.
    \item Falla en el \textbf{Middleware RabbitMQ}.
    \item Falla en \textbf{todos los Health Checkers simultáneamente}.
\end{itemize}

El manejo para generar la recuperación de las consultas, ante alguno de estos escenarios fue desestimado en esta versión del sistema, dado que implicaría definir contratos adicionales con los clientes, considerar hipótesis extremadamente remotas y un nivel de complejidad que excede los requerimientos del presente enunciado.  

Sin embargo, se documentan estos casos para su consideración en futuras versiones del sistema.

De igual manera, la ocurrencia de alguno de los tipos de falla mencionados se encuentran controlados de forma que el Sistema Distribuido pueda seguir su ejecución, luego de controlar las inconsistencias (Por ejemplo, vaciando los estados de los controladores para reiniciar consultas).

\subsubsection{Mecanismos adicionales de consistencia}

Para garantizar la coherencia incluso en escenarios de fallas simultáneas:
\begin{itemize}
    \item El nodo siguiente a uno caído valida que los mensajes recibidos no sean duplicados, comparando el último mensaje recibido con el nuevo.
    \item Los nodos que mantienen estado lo recomponen desde su último backup; los que no, reprocesan los mensajes pendientes desde la cola.
    \item Dado que \textit{RabbitMQ} no pierde mensajes durante las caídas, el sistema mantiene su integridad global.
\end{itemize}

\subsection{Sistema de Generación de Errores}

Con el objetivo de validar los mecanismos de tolerancia a fallas implementados en el sistema, se desarrolló un \textbf{Sistema de Generación de Errores}, diseñado para simular distintos escenarios de falla de manera controlada o aleatoria.

Este sistema cuenta con dos funcionalidades principales:

\begin{itemize}
    \item \textbf{Inyección manual de fallas en instancias específicas:}  
    Esta herramienta permite provocar la caída de instancias puntuales del sistema de forma controlada.  
    Su propósito principal es facilitar las pruebas durante el desarrollo, permitiendo analizar el comportamiento del sistema ante la falla y posterior recuperación de nodos concretos, así como verificar la consistencia de los datos y estados luego de cada reinicio.

    \item \textbf{Módulo de fallas aleatorias tipo \textit{Chaos Monkey}:}  
    Inspirado en las prácticas de ingeniería del caos, este módulo genera caídas y perturbaciones de manera aleatoria en distintas partes del sistema.  
    De esta forma, se evalúa el comportamiento integral del sistema distribuido bajo condiciones de falla no determinísticas, más cercanas a un entorno real de producción.  
    Este enfoque permite demostrar la robustez global y la correcta recuperación de los componentes frente a fallas imprevistas.
\end{itemize}

La señal que envía el sistema de generación de fallas a los nodos vivos es \textbf{SIGKILL}. 

En conjunto, ambas funcionalidades permiten validar tanto el funcionamiento individual de los mecanismos de tolerancia a fallas como su efectividad a nivel global, asegurando la resiliencia y estabilidad del sistema frente a distintos tipos de incidentes.

\newpage

\subsection{Mediciones de Rendimiento (Con Fallas)}

Con el objetivo de evaluar el comportamiento y la eficiencia del sistema distribuido, se realizaron diversas mediciones de rendimiento bajo distintos escenarios y volúmenes de datos.  

El propósito de este análisis es determinar la capacidad del sistema para mantener la consistencia de los resultados y la estabilidad de los tiempos de respuesta ante la presencia de fallas controladas o aleatorias.

Las pruebas se dividen en dos grupos principales: Aquellas realizadas sobre un \textbf{Dataset reducido}, orientadas a validar la consistencia frente a fallas aleatorias, y aquellas efectuadas sobre el \textbf{Dataset completo}.

\subsubsection{Dataset Reducido: Consistencia frente a caídas aleatorias}

En este conjunto de pruebas se busca demostrar que el sistema mantiene resultados consistentes ante la ocurrencia de fallas aleatorias, validando así la efectividad de los mecanismos de tolerancia a fallas implementados.

\paragraph{Herramienta de evaluación:}

Para la ejecución de estas pruebas se empleará el \textbf{Sistema de Generación de Errores} previamente descripto, en su modo de funcionamiento aleatorio tipo \textit{Chaos Monkey}.  

Esta herramienta permitirá simular caídas no determinísticas de distintas instancias del sistema, abarcando tanto nodos \textit{stateless} como \textit{stateful} y componentes del anillo de \textit{Health Checkers}.

\paragraph{Parámetros de prueba:}

Los principales parámetros de configuración de las pruebas son los siguientes (valores a definir en futuras iteraciones):

\begin{itemize}
    \item Frecuencia de generación de fallas: \texttt{30 segundos}.
    \item Cantidad de nodos involucrados: \texttt{69}.
    \item Volumen de datos procesado: \texttt{Dataset reducido}.
    \item Cantidad de consultas simultáneas: \texttt{5 (1 Cliente)}.
    \item Tamaño de los Batches: \texttt{200 líneas}.
    \item Duración promedio total de la prueba: \texttt{65 minutos}.
\end{itemize}

\paragraph{Criterios de evaluación:}
Los criterios utilizados para validar la consistencia y estabilidad del sistema serán:

\begin{itemize}
    \item Coincidencia de los resultados obtenidos con los esperados (control de consistencia).
    \item Tiempo promedio de recuperación ante fallas.
    \item Número de tareas reintentadas correctamente.
    \item Impacto en la latencia promedio del sistema.
\end{itemize}

\paragraph{Comparación con escenario sin fallas:}

Adicionalmente, se realizó la medición sobre el rendimiento del sistema con la implementación de tolerancia a fallas, pero sin la generación de fallas aleatorias.  
Esto para generar una comparativa de como afecta al rendimiento la ocurrencia de fallas ante una implementación idéntica.
El tiempo promedio medido durante esta prueba fue de \texttt{57 minutos}.

Lo que nos permité ver que la implementación de tolerancia a fallas genera una sobrecarga del \textbf{14\%} en el tiempo de procesamiento total, en comparación con un escenario sin fallas.
Aunque es un valor arbitrario, ya que depende de la frecuencia y naturaleza de las fallas generadas, nos da una idea del impacto en performance que tiene la implementación de estos mecanismos.

\subsubsection{Dataset Completo: Monitoreo de performance con y sin fallas}

Este conjunto de pruebas se orienta a analizar el rendimiento del sistema distribuido bajo carga completa, tanto en condiciones estables (sin fallas) como durante la inyección de fallas aleatorias, con el fin de evaluar la degradación del desempeño.

\paragraph{Herramienta y entorno de medición:}

Las mediciones se realizarán utilizando herramientas de monitoreo de recursos y trazabilidad, tales como \texttt{htop}.

Esta permitirá recolectar métricas en tiempo real sobre el consumo de CPU, memoria, tráfico de red y latencia promedio de las consultas distribuidas.

\paragraph{Parámetros de prueba:}

Los principales parámetros definidos para este escenario son los siguientes (valores a completar):

\begin{itemize}
    \item Frecuencia de generación de fallas: \texttt{30 segundos}.
    \item Cantidad total de nodos en ejecución: \texttt{69}.
    \item Volumen de datos procesado: \texttt{Dataset completo}.
    \item Cantidad de consultas simultáneas: \texttt{5 (1 Cliente)}.
    \item Tamaño de los Batches: \texttt{200 líneas}.
    \item Duración de la medición: Aproximadamente se estiman \texttt{330 minutos (5.5 horas)}, no se realizó la medición en su totalidad para evitar daños en el disco de la computadora.
\end{itemize}

\paragraph{Criterios de evaluación:}

Las métricas principales a analizar serán:

\begin{itemize}
    \item Tiempo promedio de procesamiento por consulta.
    \item Throughput del sistema.
    \item Utilización promedio de recursos (CPU, memoria, red).
    \item Variación del rendimiento ante la presencia de fallas.
\end{itemize}

\paragraph{Aclaración final importante:}

Las métricas de rendimiento ante fallas obtenidas, tienen un fin principalmente cualitativo, orientado a validar la robustez del sistema y la efectividad de los mecanismos de recuperación implementados.
Pero la naturaleza no determinística de las fallas generadas impide establecer comparativas cuantitativas precisas con escenarios sin fallas, debido a la variabilidad inherente en la ocurrencia y el impacto de dichas fallas.
Lo que se busca es demostrar que, incluso bajo condiciones adversas, el sistema puede mantener la consistencia y estabilidad operativa, más que optimizar tiempos específicos de respuesta.

Y también se deja en claro que las mediciones realizadas sobre el sistema pueden variar mucho entre distintas ejecuciones, dependiendo de la naturaleza y frecuencia de las fallas inyectadas.
