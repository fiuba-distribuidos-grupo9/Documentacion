\section{Middleware}

El middleware interno constituye el pilar fundamental de la comunicación en el sistema \textit{Coffee Shop Analysis}, siendo el responsable de orquestar la interacción entre el servidor central y los múltiples nodos de procesamiento. Su arquitectura se basa en un modelo de Middleware Orientado a Mensajes (MOM), implementado mediante la herramienta \textbf{RabbitMQ}, que actúa como bróker central y mediador confiable entre los distintos procesos distribuidos, tal como se visualiza en el Diagrama de Despliegue.  

La adopción de un middleware de este tipo permite abstraer completamente la complejidad de la comunicación en red, ocultando al programador los detalles de bajo nivel relacionados con sockets, serialización de datos o control de concurrencia. En lugar de conexiones rígidas punto a punto, los componentes se comunican a través de colas y \emph{exchanges} definidos estratégicamente para cada escenario. Este enfoque incrementa notablemente la flexibilidad del sistema, ya que la incorporación de un nuevo nodo de procesamiento, o la eliminación de uno existente, no requiere modificar la lógica de negocio del resto de los participantes.  

Otro aspecto central es la asincronía. Gracias al middleware, los nodos no dependen de la disponibilidad inmediata de sus contrapartes, sino que pueden depositar mensajes en una cola para que el receptor los procese cuando corresponda. Esto aporta robustez frente a picos de carga y tolerancia a fallos parciales, ya que los datos no se pierden aunque un nodo se encuentre momentáneamente inactivo. Asimismo, las colas actúan como un mecanismo de \textit{buffering}, desacoplando el ritmo de producción y consumo de datos entre los distintos nodos.

La lógica de conexión se implementa en el componente \texttt{MQConnectionHandler}, ubicado en el paquete \texttt{shared}, que centraliza la configuración de colas, \emph{exchanges} y canales de comunicación. De este modo, se asegura un manejo uniforme y estandarizado de las operaciones de envío y recepción de mensajes. Sobre esta capa de mensajería se diseñó además un protocolo de aplicación específico que define la estructura de los mensajes para el envío de datasets, la transmisión de resultados y el manejo de errores, lo que permite garantizar la correcta interpretación de la información en todas las etapas del procesamiento.

Finalmente, para interactuar con el middleware se emplearon las interfaces provistas por la cátedra, complementadas con la implementación de pruebas unitarias exhaustivas que validaron su correcto funcionamiento bajo diferentes escenarios de carga. De esta manera, el middleware cumple con el requisito planteado en el enunciado de abstraer la comunicación entre los nodos del sistema distribuido, aportando una base sólida para la escalabilidad, el desacoplamiento y la mantenibilidad del sistema en su conjunto.

\newpage

\subsection*{Esquemas de comunicación}

Durante el desarrollo se identificaron distintos patrones de comunicación, y para cada uno se diseñó una solución particular apoyada en las herramientas de RabbitMQ. A continuación, se describen los principales casos implementados

\subsubsection*{Comunicación mediante colas dedicadas}

En los nodos escalables (Para esta versión todos los nodos de nuestro Sistema Distribuido son escalables), se definió una cola por cada instancia de nodo. Esto asegura la ausencia de \textit{race conditions} y permite direccionar la información de forma controlada, garantizando el funcionamiento correcto y balanceado del sistema.

Para decidir a qué cola enviar cada batch de datos se utilizaron dos estrategias:
\begin{enumerate}
    \item \textbf{Round Robin:} Cada emisor mantiene un contador que se incrementa en cada envío, distribuyendo los lotes de datos de forma equitativa entre las colas disponibles. Una vez alcanzado el último nodo, el contador retorna al primero.  
    Esta técnica fue utilizada en situaciones como:
    \begin{itemize}
        \item Servidor $\rightarrow$ Cleaners.
        \item Cleaners $\rightarrow$ Filters.
        \item Filters $\rightarrow$ Filters.
        \item Filters $\rightarrow$ Output Builder.
        \item Join Users $\rightarrow$ Join Stores.
        \item Filters $\rightarrow$ Map Year Month / Map Year Semester.
    \end{itemize}

    \item \textbf{Hashing por clave (Sharding):} Se empleó en los casos en que los datos debían ser dirigidos a un nodo específico según una clave.
    Por ejemplo, en la comunicación de:

    \begin{itemize}
        \item Cleaners de Usuarios $\rightarrow$ Joins de Usuarios.
    \end{itemize}
    
    La asignación de transacciones a los nodos de Join se resolvió aplicando una función hash sobre el ID de usuario.
    
    Concretamente, se utilizó el resto de la división entera del ID por la cantidad de nodos shardeados, lo que asegura que todas las transacciones de un mismo usuario lleguen al mismo nodo de Join.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{img/comunicacion1.png}
    \caption{Esquema de comunicación mediante colas dedicadas.}
\end{figure}

\newpage

\subsubsection*{Comunicación mediante exchanges}

En situaciones donde la misma información debía ser replicada en múltiples colas, se utilizó un \textbf{exchange} de RabbitMQ. Este patrón resultó especialmente útil en la \textbf{Query 2}, donde se debía dividir la información para dos subconsultas distintas: los ítems más vendidos y los que generaron mayor facturación.  

En este caso, el exchange distribuye la información procesada por el nodo \textit{Map Year Month} hacia las colas de:
\begin{itemize}
    \item Count Items.
    \item Sum Items.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{img/comunicacion2.png}
    \caption{Esquema de comunicación mediante exchanges.}
\end{figure}

\subsection*{Detección y propagación de EOF}

En la nueva versión del sistema, con soporte \textbf{MultiClient}, cada cliente opera dentro de su propio \emph{contexto de sesión}, identificado por un \texttt{session\_id} (UUID).  
Esto implica que la detección y propagación de los mensajes de fin de flujo (\texttt{EOF}) debe realizarse de forma independiente por sesión, garantizando el cierre correcto de cada flujo de datos asociado a un cliente específico.

El mecanismo general conserva la misma lógica que en la versión de un solo cliente, pero se amplía para manejar múltiples sesiones concurrentes:

\begin{itemize}
    \item Cada nodo mantiene un \textbf{registro por sesión} de los emisores de los que espera recibir \texttt{EOF}. Es decir, no sólo sabe qué nodos le preceden, sino también a qué \emph{session\_id} pertenece cada flujo.
    \item Por cada \texttt{session\_id}, el nodo espera recibir la cantidad exacta de mensajes \texttt{EOF} correspondientes a sus emisores aguas arriba.
    \item Cuando un nodo ha recibido todos los \texttt{EOF} esperados para una sesión dada, puede concluir que no habrá más datos de entrada para esa sesión en particular.
    \item En ese momento, el nodo genera y propaga su propio \texttt{EOF} —incluyendo el mismo \texttt{session\_id}— hacia todos los nodos que le suceden, utilizando el mismo esquema de enrutamiento:
    \begin{itemize}
        \item Enviando a una cola directa (1 a 1).
        \item Replicando en un \emph{exchange} compartido.
        \item Distribuyendo por \emph{Round Robin} entre varias colas consumidoras.
    \end{itemize}
    \item Este proceso ocurre de forma independiente para cada sesión activa, permitiendo que distintos clientes finalicen sus flujos en momentos diferentes sin interferir entre sí.
\end{itemize}

De esta manera, el sistema logra una \textbf{propagación de EOF por sesión}, garantizando que cada cliente tenga un cierre completo y ordenado de su pipeline de procesamiento.  
Además, los nodos pueden liberar recursos por sesión una vez propagado el último \texttt{EOF}, mejorando la eficiencia y evitando bloqueos globales entre flujos concurrentes.
